{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Student Mental Health Status Classification\n",
    "## Multiclass Classification Project\n",
    "\n",
    "**Project Goal:** Develop and deploy a machine learning model to predict student mental health stress levels (Low/Medium/High) based on behavioral and academic factors.\n",
    "\n",
    "**Dataset:** Generated synthetic dataset simulating real student mental health indicators\n",
    "\n",
    "**Models to Compare:** Naive Bayes, Logistic Regression, Decision Tree, Random Forest, XGBoost, LightGBM, CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORTS AND SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Models\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "except:\n",
    "    print(\"LightGBM not installed, will skip\")\n",
    "try:\n",
    "    import catboost as cb\n",
    "except:\n",
    "    print(\"CatBoost not installed, will skip\")\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    confusion_matrix, classification_report, roc_auc_score,\n",
    "    roc_curve, auc, ConfusionMatrixDisplay\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Model saving\n",
    "import joblib\n",
    "import pickle\n",
    "\n",
    "# Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "print(\"\u2713 All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. DATASET CREATION AND LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create synthetic dataset\n",
    "n_samples = 1000\n",
    "\n",
    "data = {\n",
    "    'sleep_hours': np.random.uniform(4, 10, n_samples),\n",
    "    'study_hours_per_day': np.random.uniform(1, 8, n_samples),\n",
    "    'social_interaction_score': np.random.randint(1, 10, n_samples),\n",
    "    'exercise_hours_per_week': np.random.uniform(0, 10, n_samples),\n",
    "    'academic_performance': np.random.uniform(50, 100, n_samples),\n",
    "    'exam_anxiety_level': np.random.randint(1, 10, n_samples),\n",
    "    'family_income_level': np.random.choice(['low', 'medium', 'high'], n_samples),\n",
    "    'caffeine_intake': np.random.uniform(0, 5, n_samples),\n",
    "    'assignment_overload': np.random.randint(1, 10, n_samples),\n",
    "    'extracurricular_activities': np.random.randint(0, 6, n_samples),\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Create target variable with some logical relationships\n",
    "stress_level = []\n",
    "for idx, row in df.iterrows():\n",
    "    # Factors that increase stress\n",
    "    stress_score = 0\n",
    "    stress_score += (10 - row['sleep_hours']) * 1.5  # Less sleep = more stress\n",
    "    stress_score += row['exam_anxiety_level'] * 1.2\n",
    "    stress_score += row['assignment_overload'] * 1.3\n",
    "    stress_score += (100 - row['academic_performance']) * 0.1  # Lower grades = more stress\n",
    "    stress_score += (10 - row['social_interaction_score']) * 0.8\n",
    "    stress_score += (10 - row['exercise_hours_per_week']) * 0.5\n",
    "    \n",
    "    # Factors that decrease stress\n",
    "    stress_score -= row['extracurricular_activities'] * 0.5\n",
    "    \n",
    "    # Add some noise\n",
    "    stress_score += np.random.normal(0, 2)\n",
    "    \n",
    "    if stress_score < 15:\n",
    "        stress_level.append('Low')\n",
    "    elif stress_score < 30:\n",
    "        stress_level.append('Medium')\n",
    "    else:\n",
    "        stress_level.append('High')\n",
    "\n",
    "df['stress_level'] = stress_level\n",
    "\n",
    "print(\"Dataset created successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. EXPLORATORY DATA ANALYSIS (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"\\n=== DATASET STATISTICS ===\")\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "print(f\"\\nMissing values:\\n{df.isnull().sum()}\")\n",
    "print(f\"\\nDuplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target variable distribution\n",
    "print(\"\\n=== TARGET VARIABLE DISTRIBUTION ===\")\n",
    "print(df['stress_level'].value_counts())\n",
    "print(f\"\\nPercentage distribution:\")\n",
    "print(df['stress_level'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Count plot\n",
    "stress_counts = df['stress_level'].value_counts()\n",
    "axes[0].bar(stress_counts.index, stress_counts.values, color=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "axes[0].set_title('Stress Level Distribution', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xlabel('Stress Level')\n",
    "for i, v in enumerate(stress_counts.values):\n",
    "    axes[0].text(i, v + 5, str(v), ha='center')\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(stress_counts.values, labels=stress_counts.index, autopct='%1.1f%%',\n",
    "             colors=['#2ecc71', '#f39c12', '#e74c3c'])\n",
    "axes[1].set_title('Stress Level Proportion', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\u2713 Class distribution is balanced - good for multiclass classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical features statistics\n",
    "print(\"\\n=== NUMERICAL FEATURES STATISTICS ===\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "print(df[numerical_cols].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "print(\"\\n=== FEATURE ANALYSIS ===\")\n",
    "\n",
    "# Encode target temporarily for correlation\n",
    "le_temp = LabelEncoder()\n",
    "df['stress_level_encoded'] = le_temp.fit_transform(df['stress_level'])\n",
    "\n",
    "# FIX: Convert numerical_cols Index to list before concatenating\n",
    "numerical_cols_list = list(numerical_cols)\n",
    "correlation = df[numerical_cols_list + ['stress_level_encoded']].corr()['stress_level_encoded'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelation with target:\")\n",
    "print(correlation)\n",
    "\n",
    "# Visualize correlations\n",
    "plt.figure(figsize=(10, 6))\n",
    "correlation.drop('stress_level_encoded')[:-1].plot(kind='barh', color='steelblue')\n",
    "plt.title('Feature Correlation with Stress Level', fontsize=12, fontweight='bold')\n",
    "plt.xlabel('Correlation Coefficient')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "df.drop('stress_level_encoded', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of key features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "axes = axes.ravel()\n",
    "\n",
    "features_to_plot = ['sleep_hours', 'study_hours_per_day', 'exam_anxiety_level', \n",
    "                     'academic_performance', 'assignment_overload', 'exercise_hours_per_week']\n",
    "\n",
    "for idx, feature in enumerate(features_to_plot):\n",
    "    axes[idx].hist(df[feature], bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "    axes[idx].set_title(f'Distribution of {feature}', fontsize=10, fontweight='bold')\n",
    "    axes[idx].set_xlabel(feature)\n",
    "    axes[idx].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. DATA PREPROCESSING AND FEATURE ENGINEERING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== DATA PREPROCESSING ===\")\n",
    "\n",
    "# 1. Handle missing values (none in this dataset)\n",
    "print(f\"\\nStep 1: Missing Values Handling\")\n",
    "print(f\"Missing values before: {df.isnull().sum().sum()}\")\n",
    "# df = df.dropna()  # Not needed here\n",
    "print(f\"Missing values after: {df.isnull().sum().sum()}\")\n",
    "print(\"\u2713 No missing values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Encode categorical features\n",
    "print(\"\\nStep 2: Categorical Feature Encoding\")\n",
    "print(f\"Categorical columns: {df.select_dtypes(include='object').columns.tolist()}\")\n",
    "\n",
    "# One-hot encode family_income_level\n",
    "df_encoded = pd.get_dummies(df, columns=['family_income_level'], drop_first=True)\n",
    "print(f\"\u2713 One-hot encoding applied for family_income_level\")\n",
    "print(f\"Shape after encoding: {df_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Prepare X and y\n",
    "print(\"\\nStep 3: Feature and Target Separation\")\n",
    "\n",
    "X = df_encoded.drop('stress_level', axis=1)\n",
    "y = df_encoded['stress_level']\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"Target shape: {y.shape}\")\n",
    "print(f\"Feature names: {X.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Encode target variable\n",
    "print(\"\\nStep 4: Target Variable Encoding\")\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "\n",
    "print(f\"Classes: {le.classes_}\")\n",
    "print(f\"Encoded mapping:\")\n",
    "for i, class_name in enumerate(le.classes_):\n",
    "    print(f\"  {class_name}: {i}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Train-test split\n",
    "print(\"\\nStep 5: Train-Test Split\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"Training set size: {X_train.shape[0]} ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test set size: {X_test.shape[0]} ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nTraining set class distribution:\")\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "for cls, count in zip(unique, counts):\n",
    "    print(f\"  Class {le.classes_[cls]}: {count} samples\")\n",
    "\n",
    "print(f\"\u2713 Stratified split ensures class balance in both sets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Feature normalization/standardization\n",
    "print(\"\\nStep 6: Feature Normalization\")\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "print(f\"\u2713 StandardScaler applied\")\n",
    "print(f\"Mean of scaled training features: {X_train_scaled.mean(axis=0)[:5].round(4)}\")\n",
    "print(f\"Std of scaled training features: {X_train_scaled.std(axis=0)[:5].round(4)}\")\n",
    "\n",
    "# Convert back to DataFrame for easier handling\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X.columns)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. MODEL TRAINING AND EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL TRAINING ===\")\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {}\n",
    "results = {}\n",
    "\n",
    "# 1. Gaussian Naive Bayes\n",
    "print(\"\\n1. Training Gaussian Naive Bayes...\")\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_scaled, y_train)\n",
    "y_pred_gnb = gnb.predict(X_test_scaled)\n",
    "models['Gaussian Naive Bayes'] = gnb\n",
    "results['Gaussian Naive Bayes'] = y_pred_gnb\n",
    "print(\"\u2713 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Logistic Regression\n",
    "print(\"2. Training Multinomial Logistic Regression...\")\n",
    "lr = LogisticRegression(multi_class='multinomial', max_iter=1000, random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "models['Logistic Regression'] = lr\n",
    "results['Logistic Regression'] = y_pred_lr\n",
    "print(\"\u2713 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Decision Tree\n",
    "print(\"3. Training Decision Tree...\")\n",
    "dt = DecisionTreeClassifier(max_depth=10, random_state=42)\n",
    "dt.fit(X_train_scaled, y_train)\n",
    "y_pred_dt = dt.predict(X_test_scaled)\n",
    "models['Decision Tree'] = dt\n",
    "results['Decision Tree'] = y_pred_dt\n",
    "print(\"\u2713 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Random Forest\n",
    "print(\"4. Training Random Forest...\")\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train_scaled, y_train)\n",
    "y_pred_rf = rf.predict(X_test_scaled)\n",
    "models['Random Forest'] = rf\n",
    "results['Random Forest'] = y_pred_rf\n",
    "print(\"\u2713 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. XGBoost\n",
    "print(\"5. Training XGBoost...\")\n",
    "xgb = XGBClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                     random_state=42, eval_metric='mlogloss', verbosity=0)\n",
    "xgb.fit(X_train_scaled, y_train)\n",
    "y_pred_xgb = xgb.predict(X_test_scaled)\n",
    "models['XGBoost'] = xgb\n",
    "results['XGBoost'] = y_pred_xgb\n",
    "print(\"\u2713 Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LightGBM\n",
    "try:\n",
    "    print(\"6. Training LightGBM...\")\n",
    "    lgbm = lgb.LGBMClassifier(n_estimators=100, max_depth=6, learning_rate=0.1, \n",
    "                               random_state=42, verbose=-1)\n",
    "    lgbm.fit(X_train_scaled, y_train)\n",
    "    y_pred_lgbm = lgbm.predict(X_test_scaled)\n",
    "    models['LightGBM'] = lgbm\n",
    "    results['LightGBM'] = y_pred_lgbm\n",
    "    print(\"\u2713 Completed\")\n",
    "except:\n",
    "    print(\"\u26a0 LightGBM not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. CatBoost\n",
    "try:\n",
    "    print(\"7. Training CatBoost...\")\n",
    "    catb = cb.CatBoostClassifier(iterations=100, max_depth=6, learning_rate=0.1,\n",
    "                                   random_state=42, verbose=False)\n",
    "    catb.fit(X_train_scaled, y_train)\n",
    "    y_pred_catb = catb.predict(X_test_scaled)\n",
    "    models['CatBoost'] = catb\n",
    "    results['CatBoost'] = y_pred_catb\n",
    "    print(\"\u2713 Completed\")\n",
    "except:\n",
    "    print(\"\u26a0 CatBoost not available\")\n",
    "\n",
    "print(f\"\\n\u2713 All {len(models)} models trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. MODEL EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== MODEL EVALUATION ===\")\n",
    "\n",
    "# Compute metrics for all models\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name, y_pred in results.items():\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision_macro = precision_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    recall_macro = recall_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_macro = f1_score(y_test, y_pred, average='macro', zero_division=0)\n",
    "    f1_weighted = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
    "    \n",
    "    evaluation_results[model_name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision (Macro)': precision_macro,\n",
    "        'Recall (Macro)': recall_macro,\n",
    "        'F1 (Macro)': f1_macro,\n",
    "        'F1 (Weighted)': f1_weighted\n",
    "    }\n",
    "\n",
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(evaluation_results).T\n",
    "print(\"\\n\" + results_df.to_string())\n",
    "\n",
    "# Find best model\n",
    "best_model_name = results_df['F1 (Weighted)'].idxmax()\n",
    "print(f\"\\n\ud83c\udfc6 Best Model: {best_model_name}\")\n",
    "print(f\"   F1-Score (Weighted): {results_df.loc[best_model_name, 'F1 (Weighted)']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize model comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy comparison\n",
    "results_df['Accuracy'].sort_values(ascending=True).plot(kind='barh', ax=axes[0], color='steelblue')\n",
    "axes[0].set_title('Model Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Accuracy')\n",
    "axes[0].set_xlim([0, 1])\n",
    "for i, v in enumerate(results_df['Accuracy'].sort_values(ascending=True).values):\n",
    "    axes[0].text(v + 0.02, i, f'{v:.4f}', va='center')\n",
    "\n",
    "# F1-Score comparison\n",
    "results_df['F1 (Weighted)'].sort_values(ascending=True).plot(kind='barh', ax=axes[1], color='green')\n",
    "axes[1].set_title('Model F1-Score (Weighted) Comparison', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('F1-Score')\n",
    "axes[1].set_xlim([0, 1])\n",
    "for i, v in enumerate(results_df['F1 (Weighted)'].sort_values(ascending=True).values):\n",
    "    axes[1].text(v + 0.02, i, f'{v:.4f}', va='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. DETAILED ANALYSIS OF BEST MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n=== DETAILED ANALYSIS: {best_model_name} ===\")\n",
    "\n",
    "y_pred_best = results[best_model_name]\n",
    "best_model = models[best_model_name]\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=le.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, y_pred_best)\n",
    "print(cm)\n",
    "\n",
    "# Visualize confusion matrix\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "plt.title(f'Confusion Matrix - {best_model_name}', fontsize=12, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance (if available)\n",
    "if hasattr(best_model, 'feature_importances_'):\n",
    "    feature_importance = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': best_model.feature_importances_\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 10 Most Important Features:\")\n",
    "    print(feature_importance.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualize\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(range(10), feature_importance['Importance'].head(10).values, color='steelblue')\n",
    "    plt.yticks(range(10), feature_importance['Feature'].head(10).values)\n",
    "    plt.xlabel('Importance Score')\n",
    "    plt.title(f'Top 10 Feature Importances - {best_model_name}', fontsize=12, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. MODEL PERSISTENCE (SAVING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== SAVING MODELS ===\")\n",
    "\n",
    "import os\n",
    "\n",
    "# Create directory for models\n",
    "model_dir = 'ml_models'\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "# Save best model\n",
    "model_path = os.path.join(model_dir, f'{best_model_name.replace(\" \", \"_\")}_model.pkl')\n",
    "joblib.dump(best_model, model_path)\n",
    "print(f\"\u2713 Best model saved: {model_path}\")\n",
    "\n",
    "# Save scaler\n",
    "scaler_path = os.path.join(model_dir, 'scaler.pkl')\n",
    "joblib.dump(scaler, scaler_path)\n",
    "print(f\"\u2713 Scaler saved: {scaler_path}\")\n",
    "\n",
    "# Save label encoder\n",
    "encoder_path = os.path.join(model_dir, 'label_encoder.pkl')\n",
    "joblib.dump(le, encoder_path)\n",
    "print(f\"\u2713 Label encoder saved: {encoder_path}\")\n",
    "\n",
    "# Save feature names\n",
    "features_path = os.path.join(model_dir, 'feature_names.pkl')\n",
    "joblib.dump(X.columns.tolist(), features_path)\n",
    "print(f\"\u2713 Feature names saved: {features_path}\")\n",
    "\n",
    "# Save all models\n",
    "all_models_path = os.path.join(model_dir, 'all_models.pkl')\n",
    "joblib.dump(models, all_models_path)\n",
    "print(f\"\u2713 All models saved: {all_models_path}\")\n",
    "\n",
    "print(f\"\\n\u2713 All artifacts saved to '{model_dir}' directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. MODEL INFERENCE AND TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== LOADING AND TESTING SAVED MODEL ===\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = joblib.load(model_path)\n",
    "loaded_scaler = joblib.load(scaler_path)\n",
    "loaded_encoder = joblib.load(encoder_path)\n",
    "loaded_features = joblib.load(features_path)\n",
    "\n",
    "print(\"\u2713 All artifacts loaded successfully\")\n",
    "\n",
    "# Test on some samples\n",
    "print(\"\\nTesting on sample predictions:\")\n",
    "test_samples = X_test_scaled.head(5)\n",
    "\n",
    "predictions = loaded_model.predict(test_samples)\n",
    "predictions_proba = loaded_model.predict_proba(test_samples)\n",
    "\n",
    "for i in range(len(test_samples)):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  Predicted class: {loaded_encoder.classes_[predictions[i]]}\")\n",
    "    print(f\"  Confidence: {predictions_proba[i].max():.4f}\")\n",
    "    print(f\"  Class probabilities:\")\n",
    "    for j, cls in enumerate(loaded_encoder.classes_):\n",
    "        print(f\"    {cls}: {predictions_proba[i][j]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. SUMMARY AND CONCLUSIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PROJECT SUMMARY - STUDENT MENTAL HEALTH CLASSIFICATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\\n\ud83d\udcca DATASET INFORMATION:\")\n",
    "print(f\"  \u2022 Total samples: {len(df)}\")\n",
    "print(f\"  \u2022 Features: {X.shape[1]}\")\n",
    "print(f\"  \u2022 Classes: {len(le.classes_)} - {', '.join(le.classes_)}\")\n",
    "print(f\"  \u2022 Train-test split: 80-20\")\n",
    "\n",
    "print(f\"\\n\ud83d\udd27 PREPROCESSING STEPS:\")\n",
    "print(f\"  \u2713 Missing value handling\")\n",
    "print(f\"  \u2713 Categorical feature encoding (One-Hot)\")\n",
    "print(f\"  \u2713 Feature normalization (StandardScaler)\")\n",
    "print(f\"  \u2713 Stratified train-test split\")\n",
    "\n",
    "print(f\"\\n\ud83e\udd16 MODELS TRAINED: {len(models)}\")\n",
    "for name in models.keys():\n",
    "    print(f\"  \u2022 {name}\")\n",
    "\n",
    "print(f\"\\n\ud83c\udfc6 BEST MODEL: {best_model_name}\")\n",
    "print(f\"  \u2022 Accuracy: {evaluation_results[best_model_name]['Accuracy']:.4f}\")\n",
    "print(f\"  \u2022 Precision (Macro): {evaluation_results[best_model_name]['Precision (Macro)']:.4f}\")\n",
    "print(f\"  \u2022 Recall (Macro): {evaluation_results[best_model_name]['Recall (Macro)']:.4f}\")\n",
    "print(f\"  \u2022 F1-Score (Weighted): {evaluation_results[best_model_name]['F1 (Weighted)']:.4f}\")\n",
    "\n",
    "print(f\"\\n\ud83d\udcbe SAVED ARTIFACTS:\")\n",
    "print(f\"  \u2713 Trained model: {model_path}\")\n",
    "print(f\"  \u2713 Scaler: {scaler_path}\")\n",
    "print(f\"  \u2713 Label encoder: {encoder_path}\")\n",
    "print(f\"  \u2713 Feature names: {features_path}\")\n",
    "print(f\"  \u2713 All models: {all_models_path}\")\n",
    "\n",
    "print(f\"\\n\u2705 PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. NEXT STEPS: WEB DEPLOYMENT (FastAPI)\n",
    "\n",
    "The following code creates a FastAPI application for model deployment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save FastAPI deployment code to separate file\n",
    "fastapi_code = '''from fastapi import FastAPI, HTTPException\nfrom pydantic import BaseModel\nimport joblib\nimport numpy as np\nimport pandas as pd\nfrom typing import List\n\napp = FastAPI(title=\"Student Mental Health Classifier\", version=\"1.0.0\")\n\n# Load model and preprocessing objects\nmodel = joblib.load('ml_models/XGBoost_model.pkl')\nscaler = joblib.load('ml_models/scaler.pkl')\nencoder = joblib.load('ml_models/label_encoder.pkl')\nfeature_names = joblib.load('ml_models/feature_names.pkl')\n\nclass PredictionInput(BaseModel):\n    sleep_hours: float\n    study_hours_per_day: float\n    social_interaction_score: int\n    exercise_hours_per_week: float\n    academic_performance: float\n    exam_anxiety_level: int\n    family_income_level: str  # 'low', 'medium', 'high'\n    caffeine_intake: float\n    assignment_overload: int\n    extracurricular_activities: int\n\nclass PredictionOutput(BaseModel):\n    stress_level: str\n    confidence: float\n    probabilities: dict\n\n@app.get(\"/\")\ndef read_root():\n    return {\n        \"message\": \"Student Mental Health Classifier API\",\n        \"version\": \"1.0.0\",\n        \"endpoints\": {\n            \"predict\": \"/predict\",\n            \"health\": \"/health\"\n        }\n    }\n\n@app.get(\"/health\")\ndef health_check():\n    return {\"status\": \"healthy\"}\n\n@app.post(\"/predict\", response_model=PredictionOutput)\ndef predict(input_data: PredictionInput):\n    try:\n        # Create DataFrame with one-hot encoded features\n        data_dict = input_data.dict()\n        \n        # Handle one-hot encoding for family_income_level\n        income_level = data_dict.pop('family_income_level')\n        data_dict['family_income_level_high'] = 1 if income_level == 'high' else 0\n        data_dict['family_income_level_medium'] = 1 if income_level == 'medium' else 0\n        \n        # Create DataFrame\n        df_input = pd.DataFrame([data_dict])\n        \n        # Ensure column order matches training\n        df_input = df_input[feature_names]\n        \n        # Scale features\n        scaled_input = scaler.transform(df_input)\n        \n        # Make prediction\n        prediction = model.predict(scaled_input)[0]\n        probabilities = model.predict_proba(scaled_input)[0]\n        \n        # Get stress level and confidence\n        stress_level = encoder.classes_[prediction]\n        confidence = probabilities.max()\n        \n        # Create probability dictionary\n        prob_dict = {\n            encoder.classes_[i]: float(probabilities[i]) \n            for i in range(len(encoder.classes_))\n        }\n        \n        return PredictionOutput(\n            stress_level=stress_level,\n            confidence=confidence,\n            probabilities=prob_dict\n        )\n    \n    except Exception as e:\n        raise HTTPException(status_code=400, detail=str(e))\n\nif __name__ == \"__main__\":\n    import uvicorn\n    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n'''\n\nwith open('app.py', 'w') as f:\n",
    "    f.write(fastapi_code)\n\nprint(\"\u2713 FastAPI application code saved to 'app.py'\")\nprint(\"\\nTo run the API:\")\nprint(\"  1. Install FastAPI and Uvicorn: pip install fastapi uvicorn\")\nprint(\"  2. Run: python app.py\")\nprint(\"  3. Access API at: http://localhost:8000\")\nprint(\"  4. View API docs at: http://localhost:8000/docs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create requirements.txt for deployment\nrequirements = '''pandas==1.5.3\nnumpy==1.24.3\nscikit-learn==1.3.0\nxgboost==2.0.0\nlightgbm==4.0.0\ncatboost==1.2.0\njoblib==1.3.0\nmatplotlib==3.7.2\nseaborn==0.12.2\nfastapi==0.104.1\nuvicorn==0.24.0\npydantic==2.4.2\n'''\n\nwith open('requirements.txt', 'w') as f:\n    f.write(requirements)\n\nprint(\"\u2713 requirements.txt created\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_name": "extension",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}